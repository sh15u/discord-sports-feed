#!/usr/bin/env python3
import os, json, hashlib, argparse, time
from datetime import datetime, timedelta
from dateutil import parser as dtparse
import pytz
import feedparser
from feedgen.feed import FeedGenerator

# --- Tunables ---------------------------------------------------------------
TITLE_MAX = 70   # max chars shown in item title
DESC_MAX  = 120  # max chars shown in item description
JST = pytz.timezone("Asia/Tokyo")
# ---------------------------------------------------------------------------

def load_config(path: str):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def normalize_dt(dt_str: str):
    if not dt_str:
        return None
    try:
        dt = dtparse.parse(dt_str)
        if not dt.tzinfo:
            dt = pytz.utc.localize(dt)
        return dt.astimezone(JST)
    except Exception:
        return None

def make_guid(text: str) -> str:
    return hashlib.sha1(text.encode("utf-8")).hexdigest()

def shorten(s: str, n: int):
    s = (s or "").strip()
    return s if len(s) <= n else s[: max(0, n - 1)].rstrip() + "â€¦"

def collect_items(cfg):
    seen = set()
    items = []
    for feed in cfg["feeds"]:
        url = feed["url"]
        sport = feed["sport"]
        target_url = feed["target_url"]
        name = feed.get("name", sport.upper())
        parsed = feedparser.parse(url)
        for e in parsed.entries:
            title = getattr(e, "title", "").strip()
            link = getattr(e, "link", "").strip()
            summary = getattr(e, "summary", "").strip() if hasattr(e, "summary") else ""
            published = e.get("published") or e.get("updated") or ""
            sig = (link or "") + "||" + title
            if sig in seen:
                continue
            seen.add(sig)
            pub_dt = normalize_dt(published) or datetime.now(JST)
            items.append({
                "raw_title": title,
                "link": link or url,
                "summary": summary,
                "published": pub_dt,
                "sport": sport,
                "bet_url": target_url,
                "source_name": name
            })
    items.sort(key=lambda x: x["published"], reverse=True)
    return items

def collect_demo_items(cfg, per_sport=3):
    now = datetime.now(JST)
    demo_titles = {
        "npb": ["é˜ªç¥ vs å·¨äºº ãã‚‡ã†18:00 å…ˆç™ºç™ºè¡¨", "åºƒå³¶ãŒæ¥æˆ¦ã‚’åˆ¶ã™ã€çµ‚ç›¤ã§é€†è»¢", "ãƒ‘ãƒ»ãƒªãƒ¼ã‚°æŠ•æ‰‹æˆ¦ æ³¨ç›®ãƒã‚¤ãƒ³ãƒˆ"],
        "jleague": ["æµ¦å’Œ vs å·å´F ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", "ç¥æˆ¸ã€é¦–ä½æ”»é˜²æˆ¦ã‚’åˆ¶ã™", "æ¨ªæµœFM æ–°æˆ¦åŠ›ãŒèºå‹•"],
        "keiba": ["ã‚»ãƒ³ãƒˆãƒ©ã‚¤ãƒˆè¨˜å¿µ å±•æœ›", "é‡è³ãƒˆãƒªãƒ—ãƒ«ãƒˆãƒ¬ãƒ³ãƒ‰ï¼šæ³¨ç›®é¦¬3é ­", "ä»Šé€±ã®è¿½ã„åˆ‡ã‚Šè©•ä¾¡"],
        "mlb": ["ãƒ‰ã‚¸ãƒ£ãƒ¼ã‚¹ å¤§è°·ãŒãƒãƒ«ãƒå®‰æ‰“", "ãƒ‘ãƒ‰ãƒ¬ã‚¹ ãƒ€ãƒ«ãƒ“ãƒƒã‚·ãƒ¥å¾©å¸°ç™»æ¿", "ã‚«ãƒ–ã‚¹ éˆ´æœ¨èª ä¹ŸãŒæ±ºå‹æ‰“"]
    }
    items = []
    for feed in cfg["feeds"]:
        sport = feed["sport"]
        target_url = feed["target_url"]
        name = feed.get("name", sport.upper())
        titles = demo_titles.get(sport, [f"{sport.upper()} Demo News"])
        for i, t in enumerate(titles[:per_sport]):
            pub_dt = now - timedelta(minutes=(i * 7))
            items.append({
                "raw_title": t,
                "link": "https://example.com/demo-article",
                "summary": "ï¼ˆãƒ‡ãƒ¢ï¼‰ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¦ç´„ã§ã™ã€‚å®Ÿé‹ç”¨ã§ã¯å®Ÿéš›ã®è¨˜äº‹ã®æ¦‚è¦ãŒå…¥ã‚Šã¾ã™ã€‚",
                "published": pub_dt,
                "sport": sport,
                "bet_url": target_url,
                "source_name": name
            })
    items.sort(key=lambda x: x["published"], reverse=True)
    return items

def write_feed(outpath, title, link, description, items, emoji_by_sport,
               guid_suffix="", limit_per_sport=3, cta_text="ãƒ™ãƒƒãƒˆã¯ã“ã¡ã‚‰"):
    # cap items per sport to reduce spam
    capped, counts = [], {}
    for it in items:
        k = it["sport"]
        counts[k] = counts.get(k, 0) + 1
        if counts[k] <= limit_per_sport:
            capped.append(it)

    fg = FeedGenerator()
    fg.id(link)
    fg.title(title)
    fg.link(href=link, rel='self')
    fg.language("ja")
    fg.description(description)

    for it in capped:
        sport = it["sport"]
        emoji = emoji_by_sport.get(sport, "ğŸ²")
        display_title = f"{emoji} [{it['source_name']}] {shorten(it['raw_title'], TITLE_MAX)}"
        summary_short = shorten(it["summary"], DESC_MAX)

        fe = fg.add_entry()
        guid_seed = it["link"] + "||" + it["raw_title"] + "||" + guid_suffix
        fe.id(make_guid(guid_seed))
        fe.title(display_title)
        fe.link(href=it["link"])

        # One CTA at the TOP (bold+underline) as a markdown link â†’ no image preview
        cta_top = f"{emoji} __**[{cta_text}]({it['bet_url']})**__"

        # Keep embeds off: also add a markdown article link (no preview)
        desc_lines = [cta_top]
        if summary_short:
            desc_lines.append(summary_short)
        desc_lines.append(f"ğŸ“° [è¨˜äº‹ã‚’èª­ã‚€]({it['link']})")

        fe.description("\n\n".join(desc_lines))
        fe.pubDate(it["published"])

    os.makedirs(os.path.dirname(outpath), exist_ok=True)
    fg.rss_file(outpath, pretty=True, encoding="utf-8")
    print(f"Wrote {outpath} ({len(capped)} items)")

def main():
    p = argparse.ArgumentParser(description="JP Sports Enriched RSS")
    p.add_argument("--demo", action="store_true", help="Generate demo items")
    p.add_argument("--per-sport", type=int, default=3, help="Max items per sport (anti-spam)")
    args = p.parse_args()

    root = os.path.dirname(os.path.abspath(__file__))
    cfg = load_config(os.path.join(root, "config.json"))
    emoji_by_sport = cfg.get("emoji_by_sport", {})
    cta_cfg = cfg.get("cta", {})
    cta_text = cta_cfg.get("text", "ãƒ™ãƒƒãƒˆã¯ã“ã¡ã‚‰")

    # Demo runs get a unique GUID suffix so MEE6 treats them as new
    guid_suffix = str(int(time.time())) if args.demo else ""

    all_items = collect_demo_items(cfg, per_sport=args.per_sport) if args.demo else collect_items(cfg)

    outdir = os.path.join(root, "dist")
    os.makedirs(outdir, exist_ok=True)

    # Combined
    write_feed(
        os.path.join(outdir, "feed.xml"),
        cfg.get("feed_title", "ã‚¹ãƒãƒ¼ãƒ„é€Ÿå ±ï¼ˆãƒ™ãƒƒãƒˆãƒªãƒ³ã‚¯ä»˜ãï¼‰"),
        cfg.get("feed_link", "https://example.com/feed.xml"),
        cfg.get("feed_description", ""),
        all_items, emoji_by_sport, guid_suffix, args.per_sport, cta_text
    )

    # Per-sport (always emit files)
    by_sport = {}
    for it in all_items:
        by_sport.setdefault(it["sport"], []).append(it)

    sport_files = {"npb": "npb.xml", "jleague": "jleague.xml", "keiba": "keiba.xml", "mlb": "mlb.xml"}
    jp_names = {"npb": "NPB", "jleague": "Jãƒªãƒ¼ã‚°", "keiba": "ç«¶é¦¬", "mlb": "MLB"}
    base_link = cfg.get("feed_link","https://example.com/feed.xml").rsplit("/",1)[0]

    for sport, filename in sport_files.items():
        items = by_sport.get(sport, [])
        title = f"{cfg.get('feed_title','ã‚¹ãƒãƒ¼ãƒ„é€Ÿå ±ï¼ˆãƒ™ãƒƒãƒˆãƒªãƒ³ã‚¯ä»˜ãï¼‰')} - {jp_names.get(sport, sport.upper())}"
        link  = f"{base_link}/{filename}"
        desc  = f"{cfg.get('feed_description','')}ï¼ˆ{jp_names.get(sport, sport.upper())}ã®ã¿ï¼‰"
        write_feed(os.path.join(outdir, filename), title, link, desc, items,
                   emoji_by_sport, guid_suffix, args.per_sport, cta_text)

if __name__ == "__main__":
    main()